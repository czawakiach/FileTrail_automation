import tkinter as tk
from tkinter import ttk, filedialog, messagebox
import pandas as pd
import numpy as np
from fuzzywuzzy import fuzz, process
import re
import os
from datetime import datetime
import threading
from collections import defaultdict


class FileTrailComparator:
    def __init__(self, root):
        self.root = root
        self.root.title("FileTrail Data Comparison Tool - Optimized")
        self.root.geometry("800x600")
        
        # File paths
        self.file_paths = {
            'filetrail': None,
            'exploration_archives': None,
            'open_works_1': None,
            'open_works_2': None,
            'open_works_3': None,
            'lines_3d_ow': None,
            'lines_3d_ea': None
        }
        
        # Data storage
        self.data = {}
        self.results = {}
        
        self.create_widgets()
        
    def create_widgets(self):
        # Main frame
        main_frame = ttk.Frame(self.root, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # Title
        title_label = ttk.Label(main_frame, text="FileTrail Data Comparison Tool - Optimized", 
                               font=("Arial", 16, "bold"))
        title_label.grid(row=0, column=0, columnspan=3, pady=(0, 20))
        
        # File selection section
        file_frame = ttk.LabelFrame(main_frame, text="File Selection", padding="10")
        file_frame.grid(row=1, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=(0, 10))
        
        # FileTrail (required)
        ttk.Label(file_frame, text="FileTrail Search Result (Required):").grid(row=0, column=0, sticky=tk.W, pady=2)
        self.filetrail_var = tk.StringVar()
        ttk.Entry(file_frame, textvariable=self.filetrail_var, width=50).grid(row=0, column=1, padx=5)
        ttk.Button(file_frame, text="Browse", 
                  command=lambda: self.browse_file('filetrail')).grid(row=0, column=2)
        
        # Exploration Archives (optional)
        ttk.Label(file_frame, text="Exploration Archives (Optional):").grid(row=1, column=0, sticky=tk.W, pady=2)
        self.exploration_var = tk.StringVar()
        ttk.Entry(file_frame, textvariable=self.exploration_var, width=50).grid(row=1, column=1, padx=5)
        ttk.Button(file_frame, text="Browse", 
                  command=lambda: self.browse_file('exploration_archives')).grid(row=1, column=2)
        
        # Open Works files (up to 3)
        ttk.Label(file_frame, text="Open Works 1 (Optional):").grid(row=2, column=0, sticky=tk.W, pady=2)
        self.open_works_1_var = tk.StringVar()
        ttk.Entry(file_frame, textvariable=self.open_works_1_var, width=50).grid(row=2, column=1, padx=5)
        ttk.Button(file_frame, text="Browse", 
                  command=lambda: self.browse_file('open_works_1')).grid(row=2, column=2)
        
        ttk.Label(file_frame, text="Open Works 2 (Optional):").grid(row=3, column=0, sticky=tk.W, pady=2)
        self.open_works_2_var = tk.StringVar()
        ttk.Entry(file_frame, textvariable=self.open_works_2_var, width=50).grid(row=3, column=1, padx=5)
        ttk.Button(file_frame, text="Browse", 
                  command=lambda: self.browse_file('open_works_2')).grid(row=3, column=2)
        
        ttk.Label(file_frame, text="Open Works 3 (Optional):").grid(row=4, column=0, sticky=tk.W, pady=2)
        self.open_works_3_var = tk.StringVar()
        ttk.Entry(file_frame, textvariable=self.open_works_3_var, width=50).grid(row=4, column=1, padx=5)
        ttk.Button(file_frame, text="Browse", 
                  command=lambda: self.browse_file('open_works_3')).grid(row=4, column=2)
        
        # 3D Lines files
        ttk.Label(file_frame, text="3D Lines OW (OpenWorks) (Optional):").grid(row=5, column=0, sticky=tk.W, pady=2)
        self.lines_3d_ow_var = tk.StringVar()
        ttk.Entry(file_frame, textvariable=self.lines_3d_ow_var, width=50).grid(row=5, column=1, padx=5)
        ttk.Button(file_frame, text="Browse", 
                  command=lambda: self.browse_file('lines_3d_ow')).grid(row=5, column=2)
        
        ttk.Label(file_frame, text="3D Lines EA (Exploration Archives) (Optional):").grid(row=6, column=0, sticky=tk.W, pady=2)
        self.lines_3d_ea_var = tk.StringVar()
        ttk.Entry(file_frame, textvariable=self.lines_3d_ea_var, width=50).grid(row=6, column=1, padx=5)
        ttk.Button(file_frame, text="Browse", 
                  command=lambda: self.browse_file('lines_3d_ea')).grid(row=6, column=2)
        
        # Settings section
        settings_frame = ttk.LabelFrame(main_frame, text="Matching Settings", padding="10")
        settings_frame.grid(row=2, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=(0, 10))
        
        # Substring length setting
        ttk.Label(settings_frame, text="Min Substring Length:").grid(row=0, column=0, sticky=tk.W, padx=5)
        self.substring_length_var = tk.IntVar(value=4)
        substring_spinbox = ttk.Spinbox(settings_frame, from_=3, to=10, width=5, 
                                       textvariable=self.substring_length_var)
        substring_spinbox.grid(row=0, column=1, padx=5)
        ttk.Label(settings_frame, text="characters").grid(row=0, column=2, sticky=tk.W, padx=5)
        
        # Fuzzy threshold setting
        ttk.Label(settings_frame, text="Fuzzy Match Threshold:").grid(row=0, column=3, sticky=tk.W, padx=(20, 5))
        self.fuzzy_threshold_var = tk.IntVar(value=70)
        fuzzy_spinbox = ttk.Spinbox(settings_frame, from_=50, to=95, width=5, 
                                   textvariable=self.fuzzy_threshold_var)
        fuzzy_spinbox.grid(row=0, column=4, padx=5)
        ttk.Label(settings_frame, text="% similarity").grid(row=0, column=5, sticky=tk.W, padx=5)
        
        # Processing section
        process_frame = ttk.LabelFrame(main_frame, text="Processing", padding="10")
        process_frame.grid(row=3, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=(0, 10))
        
        # Process button
        self.process_button = ttk.Button(process_frame, text="Process Data", 
                                        command=self.process_data_threaded)
        self.process_button.grid(row=0, column=0, pady=5)
        
        # Progress bar
        self.progress_var = tk.DoubleVar()
        self.progress_bar = ttk.Progressbar(process_frame, variable=self.progress_var, 
                                           maximum=100, length=300)
        self.progress_bar.grid(row=0, column=1, padx=10, pady=5)
        
        # Status label
        self.status_var = tk.StringVar(value="Ready")
        self.status_label = ttk.Label(process_frame, textvariable=self.status_var)
        self.status_label.grid(row=0, column=2, padx=10)
        
        # Results section
        results_frame = ttk.LabelFrame(main_frame, text="Results", padding="10")
        results_frame.grid(row=4, column=0, columnspan=3, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        
        # Results text area
        self.results_text = tk.Text(results_frame, height=15, width=80)
        scrollbar = ttk.Scrollbar(results_frame, orient=tk.VERTICAL, command=self.results_text.yview)
        self.results_text.configure(yscrollcommand=scrollbar.set)
        self.results_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        scrollbar.grid(row=0, column=1, sticky=(tk.N, tk.S))
        
        # Export button
        self.export_button = ttk.Button(main_frame, text="Export to Excel", 
                                       command=self.export_to_excel, state=tk.DISABLED)
        self.export_button.grid(row=5, column=0, columnspan=3, pady=10)
        
        # Configure grid weights
        main_frame.columnconfigure(1, weight=1)
        main_frame.rowconfigure(4, weight=1)
        results_frame.columnconfigure(0, weight=1)
        results_frame.rowconfigure(0, weight=1)
        
    def browse_file(self, file_type):
        """Browse for file selection"""
        file_path = filedialog.askopenfilename(
            title=f"Select {file_type.replace('_', ' ').title()} File",
            filetypes=[("Excel files", "*.xlsx *.xls"), ("CSV files", "*.csv"), ("All files", "*.*")]
        )
        
        if file_path:
            self.file_paths[file_type] = file_path
            
            # Update the corresponding StringVar
            if file_type == 'filetrail':
                self.filetrail_var.set(file_path)
            elif file_type == 'exploration_archives':
                self.exploration_var.set(file_path)
            elif file_type == 'open_works_1':
                self.open_works_1_var.set(file_path)
            elif file_type == 'open_works_2':
                self.open_works_2_var.set(file_path)
            elif file_type == 'open_works_3':
                self.open_works_3_var.set(file_path)
            elif file_type == 'lines_3d_ow':
                self.lines_3d_ow_var.set(file_path)
            elif file_type == 'lines_3d_ea':
                self.lines_3d_ea_var.set(file_path)
    
    def normalize_string(self, text):
        """Normalize string for comparison"""
        if pd.isna(text) or text is None:
            return ""
        
        # Convert to string and uppercase
        normalized = str(text).upper().strip()
        
        # Remove common suffixes and prefixes
        normalized = re.sub(r'_3D$', '', normalized)
        normalized = re.sub(r'^E0*', '', normalized)  # Remove leading E and zeros
        
        # Remove special characters except alphanumeric and underscores
        normalized = re.sub(r'[^A-Z0-9_]', '', normalized)
        
        return normalized
    
    def create_lookup_structures(self, data_dict):
        """Create optimized lookup structures for fast matching"""
        # Get current settings from GUI
        min_substring_length = self.substring_length_var.get()
        
        lookup_structures = {}
        
        for db_name, data_list in data_dict.items():
            # Remove empty/null values
            clean_data = [str(item) for item in data_list if pd.notna(item) and str(item).strip()]
            
            if not clean_data:
                lookup_structures[db_name] = {
                    'exact_set': set(),
                    'normalized_dict': {},
                    'substring_dict': defaultdict(list),
                    'fuzzy_list': []
                }
                continue
            
            # Create exact match set (normalized)
            exact_set = set()
            normalized_dict = {}  # normalized -> original
            substring_dict = defaultdict(list)  # substring -> list of originals
            
            for item in clean_data:
                normalized = self.normalize_string(item)
                if normalized:
                    exact_set.add(normalized)
                    normalized_dict[normalized] = item
                    
                    # Create substrings for substring matching (using GUI setting)
                    if len(normalized) >= min_substring_length:
                        for i in range(len(normalized) - min_substring_length + 1):
                            substring = normalized[i:i + min_substring_length]
                            if len(substring) >= min_substring_length:
                                substring_dict[substring].append(item)
            
            lookup_structures[db_name] = {
                'exact_set': exact_set,
                'normalized_dict': normalized_dict,
                'substring_dict': substring_dict,
                'fuzzy_list': clean_data  # Keep original for fuzzy matching
            }
        
        return lookup_structures
    
    def find_matches_optimized(self, line_id, lookup_structures):
        """Optimized matching using pre-built lookup structures"""
        if not line_id or pd.isna(line_id):
            return {}
        
        # Get current settings from GUI
        min_substring_length = self.substring_length_var.get()
        fuzzy_threshold = self.fuzzy_threshold_var.get()
        
        normalized_line_id = self.normalize_string(line_id)
        if not normalized_line_id:
            return {}
        
        matches = {}
        
        for db_name, structures in lookup_structures.items():
            match_found = False
            
            # 1. Exact match (fastest)
            if normalized_line_id in structures['exact_set']:
                matches[db_name] = {
                    'value': structures['normalized_dict'][normalized_line_id],
                    'match_type': 'exact',
                    'score': 100
                }
                match_found = True
                continue
            
            # 2. Substring match (fast)
            if not match_found and len(normalized_line_id) >= min_substring_length:
                best_substring_match = None
                best_substring_score = 0
                
                # Check if any substring of line_id matches
                for i in range(len(normalized_line_id) - min_substring_length + 1):
                    substring = normalized_line_id[i:i + min_substring_length]
                    if substring in structures['substring_dict']:
                        for candidate in structures['substring_dict'][substring]:
                            # Verify it's actually a substring match
                            normalized_candidate = self.normalize_string(candidate)
                            if (normalized_line_id in normalized_candidate or 
                                normalized_candidate in normalized_line_id):
                                # Calculate better score based on length similarity
                                score = min(len(normalized_line_id), len(normalized_candidate)) / max(len(normalized_line_id), len(normalized_candidate)) * 90
                                if score > best_substring_score:
                                    best_substring_score = score
                                    best_substring_match = candidate
                
                if best_substring_match:
                    matches[db_name] = {
                        'value': best_substring_match,
                        'match_type': 'substring',
                        'score': int(best_substring_score)
                    }
                    match_found = True
                    continue
            
            # 3. Fuzzy match (slowest, only if no other match found)
            if not match_found and structures['fuzzy_list']:
                # Use process.extractOne for faster fuzzy matching
                result = process.extractOne(
                    normalized_line_id, 
                    [self.normalize_string(item) for item in structures['fuzzy_list']],
                    scorer=fuzz.ratio,
                    score_cutoff=fuzzy_threshold
                )
                
                if result:
                    matched_normalized, score = result
                    # Find original value
                    for original in structures['fuzzy_list']:
                        if self.normalize_string(original) == matched_normalized:
                            matches[db_name] = {
                                'value': original,
                                'match_type': 'fuzzy',
                                'score': score
                            }
                            break
        
        return matches
    
    def get_display_name(self, file_type):
        """Get user-friendly display name for file types"""
        if file_type == 'exploration_archives':
            return 'Exploration Archives'
        elif file_type.startswith('open_works'):
            return file_type.replace('_', ' ').replace('works', 'Works').title()
        elif file_type == 'lines_3d_ow':
            return '3D Lines OW (OpenWorks)'
        elif file_type == 'lines_3d_ea':
            return '3D Lines EA (Exploration Archives)'
        else:
            return file_type.replace('_', ' ').title()
    
    def load_data(self, file_path):
        """Load data from CSV or Excel file"""
        if not file_path or not os.path.exists(file_path):
            return None
        
        try:
            if file_path.endswith('.csv'):
                return pd.read_csv(file_path)
            else:
                return pd.read_excel(file_path)
        except Exception as e:
            messagebox.showerror("Error", f"Failed to load file {file_path}: {str(e)}")
            return None
    
    def process_data_threaded(self):
        """Run data processing in a separate thread"""
        self.process_button.config(state=tk.DISABLED)
        self.export_button.config(state=tk.DISABLED)
        
        thread = threading.Thread(target=self.process_data)
        thread.daemon = True
        thread.start()
    
    def process_data(self):
        """Main data processing function - OPTIMIZED"""
        try:
            # Clear previous results
            self.results_text.delete(1.0, tk.END)
            self.results = {}
            
            # Load FileTrail data (required)
            self.status_var.set("Loading FileTrail data...")
            self.progress_var.set(10)
            
            if not self.file_paths['filetrail']:
                messagebox.showerror("Error", "FileTrail file is required!")
                return
            
            filetrail_data = self.load_data(self.file_paths['filetrail'])
            if filetrail_data is None:
                return
            
            # Check for required columns
            if 'HOME_LOCATION' not in filetrail_data.columns:
                messagebox.showerror("Error", "HOME_LOCATION column not found in FileTrail file!")
                return
            
            if 'LINE_ID' not in filetrail_data.columns:
                messagebox.showerror("Error", "LINE_ID column not found in FileTrail file!")
                return
            
            # Filter by HOME_LOCATION (corrected to use dashes)
            self.status_var.set("Filtering by HOME_LOCATION...")
            self.progress_var.set(20)
            
            filtered_filetrail = filetrail_data[
                filetrail_data['HOME_LOCATION'].str.contains('PT-US-HOU-WCK', na=False, case=False)
            ].copy()
            
            total_filetrail = len(filetrail_data)
            filtered_count = len(filtered_filetrail)
            
            self.results_text.insert(tk.END, f"FileTrail Processing Results:\n")
            self.results_text.insert(tk.END, f"Total FileTrail records: {total_filetrail}\n")
            self.results_text.insert(tk.END, f"Records with PT-US-HOU-WCK: {filtered_count}\n\n")
            
            # Load comparison files
            self.status_var.set("Loading comparison files...")
            self.progress_var.set(30)
            
            comparison_data = {}
            comparison_columns = {
                'exploration_archives': 'EA Line Name',
                'open_works_1': 'LINE',
                'open_works_2': 'LINE',
                'open_works_3': 'LINE',
                'lines_3d_ow': 'SURVEY',
                'lines_3d_ea': 'LINE_ID'
            }
            
            for file_type, column_name in comparison_columns.items():
                if self.file_paths[file_type]:
                    data = self.load_data(self.file_paths[file_type])
                    if data is not None and column_name in data.columns:
                        comparison_data[file_type] = data[column_name].dropna().tolist()
                        display_name = self.get_display_name(file_type)
                        self.results_text.insert(tk.END, f"{display_name}: {len(comparison_data[file_type])} records\n")
                    else:
                        display_name = self.get_display_name(file_type)
                        self.results_text.insert(tk.END, f"{display_name}: Column '{column_name}' not found or file error\n")
                else:
                    display_name = self.get_display_name(file_type)
                    self.results_text.insert(tk.END, f"{display_name}: Not provided\n")
            
            self.results_text.insert(tk.END, "\n")
            
            # Create optimized lookup structures
            self.status_var.set("Creating lookup structures...")
            self.progress_var.set(40)
            
            lookup_structures = self.create_lookup_structures(comparison_data)
            
            # Process matches using vectorized operations
            self.status_var.set("Processing matches (optimized)...")
            self.progress_var.set(50)
            
            # Initialize result columns
            filtered_filetrail['EA_Match'] = ""
            filtered_filetrail['EA_Match_Type'] = ""
            filtered_filetrail['EA_Score'] = 0
            filtered_filetrail['OW1_Match'] = ""
            filtered_filetrail['OW1_Match_Type'] = ""
            filtered_filetrail['OW1_Score'] = 0
            filtered_filetrail['OW2_Match'] = ""
            filtered_filetrail['OW2_Match_Type'] = ""
            filtered_filetrail['OW2_Score'] = 0
            filtered_filetrail['OW3_Match'] = ""
            filtered_filetrail['OW3_Match_Type'] = ""
            filtered_filetrail['OW3_Score'] = 0
            filtered_filetrail['3D_OW_Match'] = ""
            filtered_filetrail['3D_OW_Match_Type'] = ""
            filtered_filetrail['3D_OW_Score'] = 0
            filtered_filetrail['3D_EA_Match'] = ""
            filtered_filetrail['3D_EA_Match_Type'] = ""
            filtered_filetrail['3D_EA_Score'] = 0
            filtered_filetrail['Has_Any_Match'] = False
            
            # Track statistics
            stats = {
                'exploration_archives': {'exact': 0, 'substring': 0, 'fuzzy': 0, 'total': 0},
                'open_works_1': {'exact': 0, 'substring': 0, 'fuzzy': 0, 'total': 0},
                'open_works_2': {'exact': 0, 'substring': 0, 'fuzzy': 0, 'total': 0},
                'open_works_3': {'exact': 0, 'substring': 0, 'fuzzy': 0, 'total': 0},
                'lines_3d_ow': {'exact': 0, 'substring': 0, 'fuzzy': 0, 'total': 0},
                'lines_3d_ea': {'exact': 0, 'substring': 0, 'fuzzy': 0, 'total': 0}
            }
            
            match_details = []
            
            # Process in batches for better performance
            batch_size = 100
            total_rows = len(filtered_filetrail)
            
            for batch_start in range(0, total_rows, batch_size):
                batch_end = min(batch_start + batch_size, total_rows)
                batch_progress = 50 + (batch_start / total_rows) * 40
                self.progress_var.set(batch_progress)
                
                # Process batch
                for idx in range(batch_start, batch_end):
                    row_idx = filtered_filetrail.index[idx]
                    line_id = filtered_filetrail.loc[row_idx, 'LINE_ID']
                    
                    if pd.isna(line_id):
                        continue
                    
                    # Find matches using optimized lookup
                    matches = self.find_matches_optimized(line_id, lookup_structures)
                    
                    has_match = bool(matches)
                    filtered_filetrail.loc[row_idx, 'Has_Any_Match'] = has_match
                    
                    # Store match results
                    for file_type, match_info in matches.items():
                        stats[file_type][match_info['match_type']] += 1
                        stats[file_type]['total'] += 1
                        
                        # Store in appropriate columns
                        if file_type == 'exploration_archives':
                            filtered_filetrail.loc[row_idx, 'EA_Match'] = match_info['value']
                            filtered_filetrail.loc[row_idx, 'EA_Match_Type'] = match_info['match_type']
                            filtered_filetrail.loc[row_idx, 'EA_Score'] = match_info['score']
                        elif file_type == 'open_works_1':
                            filtered_filetrail.loc[row_idx, 'OW1_Match'] = match_info['value']
                            filtered_filetrail.loc[row_idx, 'OW1_Match_Type'] = match_info['match_type']
                            filtered_filetrail.loc[row_idx, 'OW1_Score'] = match_info['score']
                        elif file_type == 'open_works_2':
                            filtered_filetrail.loc[row_idx, 'OW2_Match'] = match_info['value']
                            filtered_filetrail.loc[row_idx, 'OW2_Match_Type'] = match_info['match_type']
                            filtered_filetrail.loc[row_idx, 'OW2_Score'] = match_info['score']
                        elif file_type == 'open_works_3':
                            filtered_filetrail.loc[row_idx, 'OW3_Match'] = match_info['value']
                            filtered_filetrail.loc[row_idx, 'OW3_Match_Type'] = match_info['match_type']
                            filtered_filetrail.loc[row_idx, 'OW3_Score'] = match_info['score']
                        elif file_type == 'lines_3d_ow':
                            filtered_filetrail.loc[row_idx, '3D_OW_Match'] = match_info['value']
                            filtered_filetrail.loc[row_idx, '3D_OW_Match_Type'] = match_info['match_type']
                            filtered_filetrail.loc[row_idx, '3D_OW_Score'] = match_info['score']
                        elif file_type == 'lines_3d_ea':
                            filtered_filetrail.loc[row_idx, '3D_EA_Match'] = match_info['value']
                            filtered_filetrail.loc[row_idx, '3D_EA_Match_Type'] = match_info['match_type']
                            filtered_filetrail.loc[row_idx, '3D_EA_Score'] = match_info['score']
                        
                        # Store detailed match info
                        display_name = self.get_display_name(file_type)
                        
                        match_details.append({
                            'FileTrail_LINE_ID': line_id,
                            'Database': display_name,
                            'Matched_Value': match_info['value'],
                            'Match_Type': match_info['match_type'],
                            'Score': match_info['score']
                        })
            
            # Store results
            self.results['filtered_filetrail'] = filtered_filetrail
            self.results['match_details'] = pd.DataFrame(match_details)
            self.results['unique_records'] = filtered_filetrail[~filtered_filetrail['Has_Any_Match']].copy()
            
            # Display statistics
            min_substring_length = self.substring_length_var.get()
            fuzzy_threshold = self.fuzzy_threshold_var.get()
            
            self.results_text.insert(tk.END, f"Match Statistics (min {min_substring_length} chars substring, {fuzzy_threshold}% fuzzy):\n")
            for file_type, stat in stats.items():
                if stat['total'] > 0:
                    display_name = self.get_display_name(file_type)
                    self.results_text.insert(tk.END, f"{display_name}: {stat['total']} matches\n")
                    self.results_text.insert(tk.END, f"  - Exact: {stat['exact']}\n")
                    self.results_text.insert(tk.END, f"  - Substring: {stat['substring']}\n")
                    self.results_text.insert(tk.END, f"  - Fuzzy: {stat['fuzzy']}\n")
            
            unique_count = len(self.results['unique_records'])
            self.results_text.insert(tk.END, f"\nUnique FileTrail records (no matches): {unique_count}\n")
            
            # Complete
            self.status_var.set("Processing complete!")
            self.progress_var.set(100)
            self.export_button.config(state=tk.NORMAL)
            
        except Exception as e:
            messagebox.showerror("Error", f"Processing failed: {str(e)}")
            self.status_var.set("Error occurred")
        
        finally:
            self.process_button.config(state=tk.NORMAL)
    
    def export_to_excel(self):
        """Export results to Excel file"""
        if not self.results:
            messagebox.showwarning("Warning", "No results to export!")
            return
        
        # Get output file path
        output_path = filedialog.asksaveasfilename(
            title="Save Results",
            defaultextension=".xlsx",
            filetypes=[("Excel files", "*.xlsx")]
        )
        
        if not output_path:
            return
        
        try:
            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                # Sheet 1: Unique FileTrail records
                self.results['unique_records'].to_excel(
                    writer, sheet_name='Unique_FileTrail_Records', index=False
                )
                
                # Sheet 2: All FileTrail records with match info
                self.results['filtered_filetrail'].to_excel(
                    writer, sheet_name='All_FileTrail_Records', index=False
                )
                
                # Sheet 3: Match details
                if not self.results['match_details'].empty:
                    self.results['match_details'].to_excel(
                        writer, sheet_name='Match_Details', index=False
                    )
            
            messagebox.showinfo("Success", f"Results exported to {output_path}")
            
        except Exception as e:
            messagebox.showerror("Error", f"Export failed: {str(e)}")


def main():
    root = tk.Tk()
    app = FileTrailComparator(root)
    root.mainloop()


if __name__ == "__main__":
    main()
