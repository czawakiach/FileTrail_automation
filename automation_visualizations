import tkinter as tk
from tkinter import ttk, filedialog, messagebox
import pandas as pd
import numpy as np
from fuzzywuzzy import fuzz, process
import re
import os
from datetime import datetime
import threading
import time
import tracemalloc
import psutil
from collections import defaultdict
from functools import lru_cache
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
from matplotlib.figure import Figure
import io


class PerformanceMonitor:
    """Simple performance monitoring class"""
    
    def __init__(self):
        self.start_time = None
        self.start_memory = None
        self.process = psutil.Process()
        
    def start_monitoring(self):
        """Start performance monitoring"""
        tracemalloc.start()
        self.start_time = time.time()
        self.start_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        
    def get_current_stats(self):
        """Get current performance statistics"""
        if self.start_time is None:
            return None
            
        elapsed_time = time.time() - self.start_time
        current_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        memory_used = current_memory - self.start_memory
        
        # Get peak memory from tracemalloc
        current, peak = tracemalloc.get_traced_memory()
        peak_mb = peak / 1024 / 1024
        
        return {
            'elapsed_time': elapsed_time,
            'memory_used': memory_used,
            'peak_memory': peak_mb,
            'current_memory': current_memory
        }
    
    def stop_monitoring(self):
        """Stop monitoring and return final stats"""
        stats = self.get_current_stats()
        tracemalloc.stop()
        return stats


class MatchQualityAnalyzer:
    """Analyzes and categorizes match quality"""
    
    # Define match quality categories
    MATCH_CATEGORIES = {
        'Perfect': (100, 100),      # Exact matches
        'Excellent': (95, 99),      # Near-perfect matches
        'Very Good': (85, 94),      # High-quality matches
        'Good': (75, 84),           # Acceptable matches
        'Fair': (65, 74),           # Marginal matches
        'Poor': (50, 64),           # Low-quality matches
        'No Match': (0, 0)          # No matches found
    }
    
    def __init__(self):
        self.analysis_results = {}
    
    def categorize_score(self, score):
        """Categorize a match score into quality categories"""
        if score == 0:
            return 'No Match'
        
        for category, (min_score, max_score) in self.MATCH_CATEGORIES.items():
            if category == 'No Match':
                continue
            if min_score <= score <= max_score:
                return category
        
        return 'Poor'  # Default for scores below 50
    
    def analyze_matches(self, filtered_filetrail):
        """Analyze match quality across all databases"""
        databases = {
            'EA': ('EA_Match', 'EA_Match_Type', 'EA_Score'),
            'OW1': ('OW1_Match', 'OW1_Match_Type', 'OW1_Score'),
            'OW2': ('OW2_Match', 'OW2_Match_Type', 'OW2_Score'),
            'OW3': ('OW3_Match', 'OW3_Match_Type', 'OW3_Score'),
            '3D_OW': ('3D_OW_Match', '3D_OW_Match_Type', '3D_OW_Score'),
            '3D_EA': ('3D_EA_Match', '3D_EA_Match_Type', '3D_EA_Score')
        }
        
        analysis = {
            'overall_categories': {cat: 0 for cat in self.MATCH_CATEGORIES.keys()},
            'match_types': {'exact': 0, 'substring': 0, 'fuzzy': 0, 'no_match': 0},
            'database_breakdown': {},
            'total_records': len(filtered_filetrail),
            'records_with_matches': 0
        }
        
        # Initialize database breakdown
        for db_name in databases.keys():
            analysis['database_breakdown'][db_name] = {
                'categories': {cat: 0 for cat in self.MATCH_CATEGORIES.keys()},
                'match_types': {'exact': 0, 'substring': 0, 'fuzzy': 0, 'no_match': 0},
                'total_matches': 0
            }
        
        # Analyze each record
        for _, row in filtered_filetrail.iterrows():
            has_any_match = False
            
            for db_name, (match_col, type_col, score_col) in databases.items():
                score = row[score_col] if pd.notna(row[score_col]) else 0
                match_type = row[type_col] if pd.notna(row[type_col]) else ''
                
                if score > 0 and match_type:
                    has_any_match = True
                    analysis['database_breakdown'][db_name]['total_matches'] += 1
                    
                    # Categorize by quality
                    category = self.categorize_score(score)
                    analysis['overall_categories'][category] += 1
                    analysis['database_breakdown'][db_name]['categories'][category] += 1
                    
                    # Count by match type
                    if match_type in analysis['match_types']:
                        analysis['match_types'][match_type] += 1
                        analysis['database_breakdown'][db_name]['match_types'][match_type] += 1
                else:
                    # No match
                    analysis['overall_categories']['No Match'] += 1
                    analysis['database_breakdown'][db_name]['categories']['No Match'] += 1
                    analysis['match_types']['no_match'] += 1
                    analysis['database_breakdown'][db_name]['match_types']['no_match'] += 1
            
            if has_any_match:
                analysis['records_with_matches'] += 1
        
        self.analysis_results = analysis
        return analysis


class DataVisualizer:
    """Creates simple matplotlib visualizations"""
    
    def __init__(self):
        # Set matplotlib to use a non-interactive backend
        plt.style.use('default')
        
    def create_visualization_frame(self, parent):
        """Create a frame to hold visualizations"""
        viz_frame = ttk.LabelFrame(parent, text="Match Quality Analysis", padding="10")
        
        # Create notebook for different chart tabs
        notebook = ttk.Notebook(viz_frame)
        notebook.pack(fill='both', expand=True)
        
        return viz_frame, notebook
    
    def create_quality_distribution_chart(self, analysis, parent):
        """Create overall quality distribution chart"""
        fig = Figure(figsize=(10, 6), dpi=80)
        
        # Overall quality distribution (Bar Chart)
        ax1 = fig.add_subplot(121)
        categories = list(analysis['overall_categories'].keys())
        counts = list(analysis['overall_categories'].values())
        
        # Color scheme for categories
        colors = ['#2E8B57', '#32CD32', '#90EE90', '#FFD700', '#FFA500', '#FF6347', '#D3D3D3']
        
        bars = ax1.bar(categories, counts, color=colors[:len(categories)])
        ax1.set_title('Match Quality Distribution')
        ax1.set_xlabel('Quality Category')
        ax1.set_ylabel('Number of Records')
        ax1.tick_params(axis='x', rotation=45)
        
        # Add value labels on bars
        for bar, count in zip(bars, counts):
            if count > 0:
                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                        str(count), ha='center', va='bottom')
        
        # Match type distribution (Pie Chart)
        ax2 = fig.add_subplot(122)
        match_types = []
        match_counts = []
        match_colors = []
        
        type_color_map = {
            'exact': '#2E8B57',
            'substring': '#FFD700', 
            'fuzzy': '#FF6347',
            'no_match': '#D3D3D3'
        }
        
        for match_type, count in analysis['match_types'].items():
            if count > 0:
                match_types.append(match_type.title())
                match_counts.append(count)
                match_colors.append(type_color_map.get(match_type, '#D3D3D3'))
        
        if match_counts:
            wedges, texts, autotexts = ax2.pie(match_counts, labels=match_types, colors=match_colors,
                                              autopct='%1.1f%%', startangle=90)
            ax2.set_title('Match Type Distribution')
        
        fig.tight_layout()
        
        # Create canvas
        canvas = FigureCanvasTkAgg(fig, parent)
        canvas.draw()
        return canvas.get_tk_widget()
    
    def create_database_comparison_chart(self, analysis, parent):
        """Create database comparison chart"""
        fig = Figure(figsize=(12, 8), dpi=80)
        
        databases = list(analysis['database_breakdown'].keys())
        categories = list(MatchQualityAnalyzer.MATCH_CATEGORIES.keys())
        
        # Create stacked bar chart
        ax = fig.add_subplot(111)
        
        # Prepare data for stacked bars
        bottom_values = np.zeros(len(databases))
        colors = ['#2E8B57', '#32CD32', '#90EE90', '#FFD700', '#FFA500', '#FF6347', '#D3D3D3']
        
        category_handles = []
        
        for i, category in enumerate(categories):
            values = [analysis['database_breakdown'][db]['categories'][category] for db in databases]
            
            if any(v > 0 for v in values):  # Only plot if there are values
                bars = ax.bar(databases, values, bottom=bottom_values, 
                             label=category, color=colors[i % len(colors)])
                category_handles.append(bars[0])
                bottom_values += np.array(values)
        
        ax.set_title('Match Quality Distribution by Database')
        ax.set_xlabel('Database')
        ax.set_ylabel('Number of Matches')
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        fig.tight_layout()
        
        canvas = FigureCanvasTkAgg(fig, parent)
        canvas.draw()
        return canvas.get_tk_widget()
    
    def create_summary_stats_widget(self, analysis, parent):
        """Create a summary statistics display"""
        stats_frame = ttk.Frame(parent)
        
        # Calculate key metrics
        total_records = analysis['total_records']
        records_with_matches = analysis['records_with_matches']
        match_rate = (records_with_matches / total_records * 100) if total_records > 0 else 0
        
        perfect_matches = analysis['overall_categories']['Perfect']
        good_or_better = (analysis['overall_categories']['Perfect'] + 
                         analysis['overall_categories']['Excellent'] + 
                         analysis['overall_categories']['Very Good'] + 
                         analysis['overall_categories']['Good'])
        
        # Create summary labels
        summary_text = f"""
Match Quality Summary:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

📊 Overall Statistics:
   • Total Records Processed: {total_records:,}
   • Records with Matches: {records_with_matches:,} ({match_rate:.1f}%)
   • Perfect Matches (100%): {perfect_matches:,}
   • Good or Better Matches (≥75%): {good_or_better:,}

🎯 Match Type Breakdown:
   • Exact Matches: {analysis['match_types']['exact']:,}
   • Substring Matches: {analysis['match_types']['substring']:,}
   • Fuzzy Matches: {analysis['match_types']['fuzzy']:,}
   • No Matches: {analysis['match_types']['no_match']:,}

🏆 Quality Categories:
   • Perfect (100%): {analysis['overall_categories']['Perfect']:,}
   • Excellent (95-99%): {analysis['overall_categories']['Excellent']:,}
   • Very Good (85-94%): {analysis['overall_categories']['Very Good']:,}
   • Good (75-84%): {analysis['overall_categories']['Good']:,}
   • Fair (65-74%): {analysis['overall_categories']['Fair']:,}
   • Poor (50-64%): {analysis['overall_categories']['Poor']:,}
   • No Match (0%): {analysis['overall_categories']['No Match']:,}
"""
        
        summary_label = tk.Text(stats_frame, height=20, width=60, font=("Consolas", 10))
        summary_label.insert('1.0', summary_text)
        summary_label.config(state='disabled')
        
        scrollbar = ttk.Scrollbar(stats_frame, orient='vertical', command=summary_label.yview)
        summary_label.configure(yscrollcommand=scrollbar.set)
        
        summary_label.pack(side='left', fill='both', expand=True)
        scrollbar.pack(side='right', fill='y')
        
        return stats_frame
    
    def save_charts_for_excel(self, analysis, output_dir):
        """Save charts as images for Excel export"""
        chart_paths = {}
        
        try:
            # Quality distribution chart
            fig1 = plt.figure(figsize=(12, 6))
            
            # Overall quality distribution
            ax1 = fig1.add_subplot(121)
            categories = list(analysis['overall_categories'].keys())
            counts = list(analysis['overall_categories'].values())
            colors = ['#2E8B57', '#32CD32', '#90EE90', '#FFD700', '#FFA500', '#FF6347', '#D3D3D3']
            
            bars = ax1.bar(categories, counts, color=colors[:len(categories)])
            ax1.set_title('Match Quality Distribution')
            ax1.set_xlabel('Quality Category')
            ax1.set_ylabel('Number of Records')
            ax1.tick_params(axis='x', rotation=45)
            
            for bar, count in zip(bars, counts):
                if count > 0:
                    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                            str(count), ha='center', va='bottom')
            
            # Match type distribution
            ax2 = fig1.add_subplot(122)
            match_types = []
            match_counts = []
            match_colors = []
            
            type_color_map = {
                'exact': '#2E8B57',
                'substring': '#FFD700', 
                'fuzzy': '#FF6347',
                'no_match': '#D3D3D3'
            }
            
            for match_type, count in analysis['match_types'].items():
                if count > 0:
                    match_types.append(match_type.title())
                    match_counts.append(count)
                    match_colors.append(type_color_map.get(match_type, '#D3D3D3'))
            
            if match_counts:
                ax2.pie(match_counts, labels=match_types, colors=match_colors,
                       autopct='%1.1f%%', startangle=90)
                ax2.set_title('Match Type Distribution')
            
            plt.tight_layout()
            
            chart1_path = os.path.join(output_dir, 'quality_distribution.png')
            plt.savefig(chart1_path, dpi=150, bbox_inches='tight')
            chart_paths['quality_distribution'] = chart1_path
            plt.close(fig1)
            
            # Database comparison chart
            fig2 = plt.figure(figsize=(12, 8))
            databases = list(analysis['database_breakdown'].keys())
            categories = list(MatchQualityAnalyzer.MATCH_CATEGORIES.keys())
            
            ax = fig2.add_subplot(111)
            bottom_values = np.zeros(len(databases))
            
            for i, category in enumerate(categories):
                values = [analysis['database_breakdown'][db]['categories'][category] for db in databases]
                
                if any(v > 0 for v in values):
                    ax.bar(databases, values, bottom=bottom_values, 
                          label=category, color=colors[i % len(colors)])
                    bottom_values += np.array(values)
            
            ax.set_title('Match Quality Distribution by Database')
            ax.set_xlabel('Database')
            ax.set_ylabel('Number of Matches')
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            
            plt.tight_layout()
            
            chart2_path = os.path.join(output_dir, 'database_comparison.png')
            plt.savefig(chart2_path, dpi=150, bbox_inches='tight')
            chart_paths['database_comparison'] = chart2_path
            plt.close(fig2)
            
        except Exception as e:
            print(f"Error saving charts: {e}")
        
        return chart_paths


class FileTrailComparator:
    def __init__(self, root):
        self.root = root
        self.root.title("FileTrail Data Comparison Tool - Enhanced with Visualization")
        
        # Make window responsive to screen size
        screen_width = root.winfo_screenwidth()
        screen_height = root.winfo_screenheight()
        window_width = min(1200, int(screen_width * 0.9))  # Slightly larger for charts
        window_height = min(900, int(screen_height * 0.9))
        
        # Center the window
        x = (screen_width - window_width) // 2
        y = (screen_height - window_height) // 2
        
        self.root.geometry(f"{window_width}x{window_height}+{x}+{y}")
        self.root.minsize(1000, 700)  # Increased minimum size
        
        # File paths
        self.file_paths = {
            'filetrail': None,
            'exploration_archives': None,
            'open_works_1': None,
            'open_works_2': None,
            'open_works_3': None,
            'lines_3d_ow': None,
            'lines_3d_ea': None
        }
        
        # Data storage and caching
        self.data = {}
        self.results = {}
        self.processing_cancelled = False
        self.performance_monitor = PerformanceMonitor()
        
        # Cache for normalized strings and matches
        self.normalize_cache = {}
        self.match_cache = {}
        
        # Analysis components
        self.match_analyzer = MatchQualityAnalyzer()
        self.data_visualizer = DataVisualizer()
        self.visualization_widgets = []
        
        self.create_widgets()
        
    def create_widgets(self):
        # Create main notebook for tabs
        main_notebook = ttk.Notebook(self.root, padding="5")
        main_notebook.pack(fill='both', expand=True)
        
        # Tab 1: File Processing
        process_frame = ttk.Frame(main_notebook, padding="10")
        main_notebook.add(process_frame, text="File Processing")
        
        # Tab 2: Visualizations (initially empty)
        self.viz_frame = ttk.Frame(main_notebook, padding="10")
        main_notebook.add(self.viz_frame, text="Analysis & Charts")
        
        # Store reference to notebook for later use
        self.main_notebook = main_notebook
        
        # Create processing interface in first tab
        self.create_processing_interface(process_frame)
        
        # Create placeholder for visualization
        self.create_visualization_placeholder()
    
    def create_processing_interface(self, parent):
        # Title
        title_label = ttk.Label(parent, text="FileTrail Data Comparison Tool - Enhanced with Visualization", 
                               font=("Arial", 16, "bold"))
        title_label.grid(row=0, column=0, columnspan=3, pady=(0, 20))
        
        # File selection section
        file_frame = ttk.LabelFrame(parent, text="File Selection", padding="10")
        file_frame.grid(row=1, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=(0, 10))
        
        # FileTrail (required)
        ttk.Label(file_frame, text="FileTrail Search Result (Required):").grid(row=0, column=0, sticky=tk.W, pady=2)
        self.filetrail_var = tk.StringVar()
        ttk.Entry(file_frame, textvariable=self.filetrail_var, width=50).grid(row=0, column=1, padx=5)
        ttk.Button(file_frame, text="Browse", 
                  command=lambda: self.browse_file('filetrail')).grid(row=0, column=2)
        
        # Exploration Archives (optional)
        ttk.Label(file_frame, text="Exploration Archives (Optional):").grid(row=1, column=0, sticky=tk.W, pady=2)
        self.exploration_var = tk.StringVar()
        ttk.Entry(file_frame, textvariable=self.exploration_var, width=50).grid(row=1, column=1, padx=5)
        ttk.Button(file_frame, text="Browse", 
                  command=lambda: self.browse_file('exploration_archives')).grid(row=1, column=2)
        
        # Open Works files (up to 3)
        ttk.Label(file_frame, text="Open Works 1 (Optional):").grid(row=2, column=0, sticky=tk.W, pady=2)
        self.open_works_1_var = tk.StringVar()
        ttk.Entry(file_frame, textvariable=self.open_works_1_var, width=50).grid(row=2, column=1, padx=5)
        ttk.Button(file_frame, text="Browse", 
                  command=lambda: self.browse_file('open_works_1')).grid(row=2, column=2)
        
        ttk.Label(file_frame, text="Open Works 2 (Optional):").grid(row=3, column=0, sticky=tk.W, pady=2)
        self.open_works_2_var = tk.StringVar()
        ttk.Entry(file_frame, textvariable=self.open_works_2_var, width=50).grid(row=3, column=1, padx=5)
        ttk.Button(file_frame, text="Browse", 
                  command=lambda: self.browse_file('open_works_2')).grid(row=3, column=2)
        
        ttk.Label(file_frame, text="Open Works 3 (Optional):").grid(row=4, column=0, sticky=tk.W, pady=2)
        self.open_works_3_var = tk.StringVar()
        ttk.Entry(file_frame, textvariable=self.open_works_3_var, width=50).grid(row=4, column=1, padx=5)
        ttk.Button(file_frame, text="Browse", 
                  command=lambda: self.browse_file('open_works_3')).grid(row=4, column=2)
        
        # 3D Lines files
        ttk.Label(file_frame, text="3D Lines OW (OpenWorks) (Optional):").grid(row=5, column=0, sticky=tk.W, pady=2)
        self.lines_3d_ow_var = tk.StringVar()
        ttk.Entry(file_frame, textvariable=self.lines_3d_ow_var, width=50).grid(row=5, column=1, padx=5)
        ttk.Button(file_frame, text="Browse", 
                  command=lambda: self.browse_file('lines_3d_ow')).grid(row=5, column=2)
        
        ttk.Label(file_frame, text="3D Lines EA (Exploration Archives) (Optional):").grid(row=6, column=0, sticky=tk.W, pady=2)
        self.lines_3d_ea_var = tk.StringVar()
        ttk.Entry(file_frame, textvariable=self.lines_3d_ea_var, width=50).grid(row=6, column=1, padx=5)
        ttk.Button(file_frame, text="Browse", 
                  command=lambda: self.browse_file('lines_3d_ea')).grid(row=6, column=2)
        
        # Settings section
        settings_frame = ttk.LabelFrame(parent, text="Matching Settings", padding="10")
        settings_frame.grid(row=2, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=(0, 10))
        
        # Substring length setting
        ttk.Label(settings_frame, text="Min Substring Length:").grid(row=0, column=0, sticky=tk.W, padx=5)
        self.substring_length_var = tk.IntVar(value=6)
        substring_spinbox = ttk.Spinbox(settings_frame, from_=4, to=12, width=5, 
                                       textvariable=self.substring_length_var)
        substring_spinbox.grid(row=0, column=1, padx=5)
        ttk.Label(settings_frame, text="characters").grid(row=0, column=2, sticky=tk.W, padx=5)
        
        # Fuzzy threshold setting
        ttk.Label(settings_frame, text="Fuzzy Match Threshold:").grid(row=0, column=3, sticky=tk.W, padx=(20, 5))
        self.fuzzy_threshold_var = tk.IntVar(value=75)
        fuzzy_spinbox = ttk.Spinbox(settings_frame, from_=50, to=95, width=5, 
                                   textvariable=self.fuzzy_threshold_var)
        fuzzy_spinbox.grid(row=0, column=4, padx=5)
        ttk.Label(settings_frame, text="% similarity").grid(row=0, column=5, sticky=tk.W, padx=5)
        
        # Processing section
        process_control_frame = ttk.LabelFrame(parent, text="Processing", padding="10")
        process_control_frame.grid(row=3, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=(0, 10))
        
        # Process and Cancel buttons
        button_frame = ttk.Frame(process_control_frame)
        button_frame.grid(row=0, column=0, pady=5)
        
        self.process_button = ttk.Button(button_frame, text="Process Data", 
                                        command=self.process_data_threaded)
        self.process_button.grid(row=0, column=0, padx=5)
        
        self.cancel_button = ttk.Button(button_frame, text="Cancel", 
                                       command=self.cancel_processing, state=tk.DISABLED)
        self.cancel_button.grid(row=0, column=1, padx=5)
        
        # Clear/Reset button
        self.clear_button = ttk.Button(button_frame, text="Clear All", 
                                      command=self.clear_all_data)
        self.clear_button.grid(row=0, column=2, padx=5)
        
        # Export button
        self.export_button = ttk.Button(button_frame, text="Export to Excel", 
                                       command=self.export_to_excel, state=tk.DISABLED)
        self.export_button.grid(row=0, column=3, padx=15)
        
        # Progress bar
        self.progress_var = tk.DoubleVar()
        self.progress_bar = ttk.Progressbar(process_control_frame, variable=self.progress_var, 
                                           maximum=100, length=300)
        self.progress_bar.grid(row=0, column=1, padx=10, pady=5)
        
        # Status label
        self.status_var = tk.StringVar(value="Ready")
        self.status_label = ttk.Label(process_control_frame, textvariable=self.status_var)
        self.status_label.grid(row=0, column=2, padx=10)
        
        # Performance info
        self.perf_var = tk.StringVar(value="")
        self.perf_label = ttk.Label(process_control_frame, textvariable=self.perf_var, font=("Arial", 8))
        self.perf_label.grid(row=1, column=0, columnspan=3, pady=5)
        
        # Results section (adjusted to give more space)
        results_frame = ttk.LabelFrame(parent, text="Processing Results", padding="10")
        results_frame.grid(row=4, column=0, columnspan=3, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        
        # Results text area
        self.results_text = tk.Text(results_frame, height=12, width=80)
        scrollbar = ttk.Scrollbar(results_frame, orient=tk.VERTICAL, command=self.results_text.yview)
        self.results_text.configure(yscrollcommand=scrollbar.set)
        self.results_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        scrollbar.grid(row=0, column=1, sticky=(tk.N, tk.S))
        
        # Configure grid weights for responsive layout
        parent.columnconfigure(1, weight=1)
        parent.rowconfigure(4, weight=1)
        results_frame.columnconfigure(0, weight=1)
        results_frame.rowconfigure(0, weight=1)
        process_control_frame.columnconfigure(1, weight=1)
    
    def create_visualization_placeholder(self):
        """Create placeholder for visualization tab"""
        placeholder_label = ttk.Label(self.viz_frame, 
                                     text="Process data first to see analysis and charts here.",
                                     font=("Arial", 12))
        placeholder_label.pack(expand=True)
        
    def create_visualizations(self, analysis):
        """Create visualization widgets after processing"""
        # Clear existing visualization widgets
        for widget in self.viz_frame.winfo_children():
            widget.destroy()
        
        # Create visualization notebook
        viz_notebook = ttk.Notebook(self.viz_frame, padding="5")
        viz_notebook.pack(fill='both', expand=True)
        
        # Tab 1: Summary Statistics
        summary_frame = ttk.Frame(viz_notebook, padding="10")
        viz_notebook.add(summary_frame, text="📊 Summary")
        summary_widget = self.data_visualizer.create_summary_stats_widget(analysis, summary_frame)
        summary_widget.pack(fill='both', expand=True)
        
        # Tab 2: Quality Distribution Charts
        quality_frame = ttk.Frame(viz_notebook, padding="10")
        viz_notebook.add(quality_frame, text="📈 Quality Distribution")
        quality_widget = self.data_visualizer.create_quality_distribution_chart(analysis, quality_frame)
        quality_widget.pack(fill='both', expand=True)
        
        # Tab 3: Database Comparison
        comparison_frame = ttk.Frame(viz_notebook, padding="10")
        viz_notebook.add(comparison_frame, text="🔍 Database Comparison")
        comparison_widget = self.data_visualizer.create_database_comparison_chart(analysis, comparison_frame)
        comparison_widget.pack(fill='both', expand=True)
        
        # Switch to visualization tab
        self.main_notebook.select(1)
        
    def browse_file(self, file_type):
        """Browse for file selection with validation"""
        file_path = filedialog.askopenfilename(
            title=f"Select {file_type.replace('_', ' ').title()} File",
            filetypes=[("Excel files", "*.xlsx *.xls"), ("CSV files", "*.csv"), ("All files", "*.*")]
        )
        
        if file_path:
            # Validate file exists and is readable
            if not os.path.exists(file_path):
                messagebox.showerror("Error", f"File does not exist: {file_path}")
                return
                
            # Quick validation - try to peek at the file
            try:
                if file_path.endswith('.csv'):
                    # Read just the header to validate
                    pd.read_csv(file_path, nrows=0)
                else:
                    # Read just the header for Excel files
                    pd.read_excel(file_path, nrows=0)
            except Exception as e:
                messagebox.showerror("Error", f"Cannot read file {file_path}:\n{str(e)}")
                return
            
            self.file_paths[file_type] = file_path
            
            # Update the corresponding StringVar
            if file_type == 'filetrail':
                self.filetrail_var.set(file_path)
            elif file_type == 'exploration_archives':
                self.exploration_var.set(file_path)
            elif file_type == 'open_works_1':
                self.open_works_1_var.set(file_path)
            elif file_type == 'open_works_2':
                self.open_works_2_var.set(file_path)
            elif file_type == 'open_works_3':
                self.open_works_3_var.set(file_path)
            elif file_type == 'lines_3d_ow':
                self.lines_3d_ow_var.set(file_path)
            elif file_type == 'lines_3d_ea':
                self.lines_3d_ea_var.set(file_path)
    
    @lru_cache(maxsize=10000)
    def normalize_string_cached(self, text):
        """Cached version of normalize_string for better performance"""
        if pd.isna(text) or text is None:
            return ""
        
        # Convert to string and uppercase
        normalized = str(text).upper().strip()
        
        # Remove common suffixes and prefixes
        normalized = re.sub(r'_3D, '', normalized)
        normalized = re.sub(r'^E0*', '', normalized)  # Remove leading E and zeros
        
        # Remove special characters except alphanumeric and underscores
        normalized = re.sub(r'[^A-Z0-9_]', '', normalized)
        
        return normalized
    
    def create_smart_substrings(self, text, min_length):
        """Create intelligent substrings based on word boundaries and patterns"""
        if not text or len(text) < min_length:
            return []
        
        substrings = set()
        
        # Strategy 1: Split by underscores and take meaningful parts
        parts = text.split('_')
        for part in parts:
            if len(part) >= min_length:
                substrings.add(part)
        
        # Strategy 2: Extract alphanumeric blocks
        alphanumeric_blocks = re.findall(r'[A-Z0-9]{' + str(min_length) + ',}', text)
        substrings.update(alphanumeric_blocks)
        
        # Strategy 3: Create overlapping substrings but limit the number
        if len(text) <= 20:  # Only for shorter strings to avoid explosion
            for i in range(0, len(text) - min_length + 1, 2):  # Step by 2 to reduce overlap
                substring = text[i:i + min_length]
                if len(substring) >= min_length:
                    substrings.add(substring)
        else:
            # For longer strings, just take beginning, middle, and end parts
            substrings.add(text[:min_length])
            if len(text) > min_length * 2:
                mid_start = len(text) // 2 - min_length // 2
                substrings.add(text[mid_start:mid_start + min_length])
            substrings.add(text[-min_length:])
        
        return list(substrings)
    
    def create_lookup_structures(self, data_dict):
        """Create optimized lookup structures with smart substring generation"""
        min_substring_length = self.substring_length_var.get()
        lookup_structures = {}
        
        for db_name, data_list in data_dict.items():
            # Remove empty/null values and limit size for memory management
            clean_data = []
            seen = set()  # Remove duplicates
            
            for item in data_list:
                if pd.notna(item) and str(item).strip():
                    item_str = str(item)
                    if item_str not in seen:
                        clean_data.append(item_str)
                        seen.add(item_str)
                        
                        # Limit to prevent memory issues
                        if len(clean_data) > 50000:  # Reasonable limit
                            break
            
            if not clean_data:
                lookup_structures[db_name] = {
                    'exact_set': set(),
                    'normalized_dict': {},
                    'substring_dict': defaultdict(set),  # Use set to avoid duplicates
                    'fuzzy_list': []
                }
                continue
            
            # Create lookup structures
            exact_set = set()
            normalized_dict = {}
            substring_dict = defaultdict(set)
            
            for item in clean_data:
                normalized = self.normalize_string_cached(item)
                if normalized:
                    exact_set.add(normalized)
                    normalized_dict[normalized] = item
                    
                    # Create smart substrings
                    substrings = self.create_smart_substrings(normalized, min_substring_length)
                    for substring in substrings:
                        substring_dict[substring].add(item)
            
            lookup_structures[db_name] = {
                'exact_set': exact_set,
                'normalized_dict': normalized_dict,
                'substring_dict': substring_dict,
                'fuzzy_list': clean_data[:1000]  # Limit fuzzy matching to prevent slowdown
            }
        
        return lookup_structures
    
    def find_matches_optimized(self, line_id, lookup_structures):
        """Optimized matching with caching"""
        if not line_id or pd.isna(line_id):
            return {}
        
        # Check cache first
        cache_key = str(line_id)
        if cache_key in self.match_cache:
            return self.match_cache[cache_key]
        
        fuzzy_threshold = self.fuzzy_threshold_var.get()
        normalized_line_id = self.normalize_string_cached(line_id)
        
        if not normalized_line_id:
            self.match_cache[cache_key] = {}
            return {}
        
        matches = {}
        
        for db_name, structures in lookup_structures.items():
            # 1. Exact match (fastest)
            if normalized_line_id in structures['exact_set']:
                matches[db_name] = {
                    'value': structures['normalized_dict'][normalized_line_id],
                    'match_type': 'exact',
                    'score': 100
                }
                continue
            
            # 2. Substring match
            best_substring_match = None
            best_substring_score = 0
            
            # Check substrings of the line_id against our substring dictionary
            min_length = self.substring_length_var.get()
            substrings = self.create_smart_substrings(normalized_line_id, min_length)
            
            for substring in substrings:
                if substring in structures['substring_dict']:
                    for candidate in structures['substring_dict'][substring]:
                        normalized_candidate = self.normalize_string_cached(candidate)
                        if (normalized_line_id in normalized_candidate or 
                            normalized_candidate in normalized_line_id):
                            score = min(len(normalized_line_id), len(normalized_candidate)) / max(len(normalized_line_id), len(normalized_candidate)) * 90
                            if score > best_substring_score:
                                best_substring_score = score
                                best_substring_match = candidate
            
            if best_substring_match:
                matches[db_name] = {
                    'value': best_substring_match,
                    'match_type': 'substring',
                    'score': int(best_substring_score)
                }
                continue
            
            # 3. Fuzzy match (limited scope)
            if structures['fuzzy_list']:
                result = process.extractOne(
                    normalized_line_id, 
                    [self.normalize_string_cached(item) for item in structures['fuzzy_list']],
                    scorer=fuzz.ratio,
                    score_cutoff=fuzzy_threshold
                )
                
                if result:
                    matched_normalized, score = result
                    for original in structures['fuzzy_list']:
                        if self.normalize_string_cached(original) == matched_normalized:
                            matches[db_name] = {
                                'value': original,
                                'match_type': 'fuzzy',
                                'score': score
                            }
                            break
        
        # Cache the result
        self.match_cache[cache_key] = matches
        return matches
    
    def get_display_name(self, file_type):
        """Get user-friendly display name for file types"""
        if file_type == 'exploration_archives':
            return 'Exploration Archives'
        elif file_type.startswith('open_works'):
            return file_type.replace('_', ' ').replace('works', 'Works').title()
        elif file_type == 'lines_3d_ow':
            return '3D Lines OW (OpenWorks)'
        elif file_type == 'lines_3d_ea':
            return '3D Lines EA (Exploration Archives)'
        else:
            return file_type.replace('_', ' ').title()
    
    def validate_input_files(self):
        """Validate input files and their required columns"""
        # Check if FileTrail file is provided
        if not self.file_paths['filetrail']:
            raise ValueError("FileTrail file is required!")
        
        # Validate FileTrail file columns
        try:
            if self.file_paths['filetrail'].endswith('.csv'):
                df_sample = pd.read_csv(self.file_paths['filetrail'], nrows=0)
            else:
                df_sample = pd.read_excel(self.file_paths['filetrail'], nrows=0)
            
            required_columns = ['HOME_LOCATION', 'LINE_ID']
            missing_columns = [col for col in required_columns if col not in df_sample.columns]
            
            if missing_columns:
                raise ValueError(f"Required columns missing from FileTrail file: {missing_columns}")
                
        except Exception as e:
            raise ValueError(f"Error validating FileTrail file: {str(e)}")
        
        # Validate comparison files
        comparison_columns = {
            'exploration_archives': 'EA Line Name',
            'open_works_1': 'LINE',
            'open_works_2': 'LINE',
            'open_works_3': 'LINE',
            'lines_3d_ow': 'SURVEY',
            'lines_3d_ea': 'LINE_ID'
        }
        
        validation_results = {}
        
        for file_type, expected_column in comparison_columns.items():
            if self.file_paths[file_type]:
                try:
                    if self.file_paths[file_type].endswith('.csv'):
                        df_sample = pd.read_csv(self.file_paths[file_type], nrows=0)
                    else:
                        df_sample = pd.read_excel(self.file_paths[file_type], nrows=0)
                    
                    if expected_column not in df_sample.columns:
                        validation_results[file_type] = f"Column '{expected_column}' not found"
                    else:
                        validation_results[file_type] = "OK"
                        
                except Exception as e:
                    validation_results[file_type] = f"Error: {str(e)}"
            else:
                validation_results[file_type] = "Not provided"
        
        return validation_results
    
    def load_data_efficiently(self, file_path, column_name=None, max_rows=None):
        """Load data efficiently with memory management"""
        if not file_path or not os.path.exists(file_path):
            return None
        
        try:
            if file_path.endswith('.csv'):
                if column_name:
                    # Only load the required column
                    df = pd.read_csv(file_path, usecols=[column_name], nrows=max_rows)
                else:
                    df = pd.read_csv(file_path, nrows=max_rows)
            else:
                if column_name:
                    df = pd.read_excel(file_path, usecols=[column_name], nrows=max_rows)
                else:
                    df = pd.read_excel(file_path, nrows=max_rows)
            
            return df
            
        except Exception as e:
            self.update_ui_safe(lambda: messagebox.showerror("Error", f"Failed to load file {file_path}: {str(e)}"))
            return None
    
    def update_ui_safe(self, callback):
        """Thread-safe UI updates"""
        if self.root.winfo_exists():
            self.root.after(0, callback)
    
    def update_performance_display(self):
        """Update performance metrics display"""
        stats = self.performance_monitor.get_current_stats()
        if stats:
            perf_text = f"Time: {stats['elapsed_time']:.1f}s | Memory: +{stats['memory_used']:.1f}MB | Peak: {stats['peak_memory']:.1f}MB"
            self.update_ui_safe(lambda: self.perf_var.set(perf_text))
    
    def clear_all_data(self):
        """Clear all loaded data, cache, and reset the interface for a fresh start"""
        # Confirm with user before clearing
        result = messagebox.askyesno(
            "Clear All Data", 
            "This will clear all loaded files, cached data, and results.\n\n"
            "Are you sure you want to continue?"
        )
        
        if not result:
            return
        
        try:
            # Clear file paths
            self.file_paths = {
                'filetrail': None,
                'exploration_archives': None,
                'open_works_1': None,
                'open_works_2': None,
                'open_works_3': None,
                'lines_3d_ow': None,
                'lines_3d_ea': None
            }
            
            # Clear all StringVar variables (file path displays)
            self.filetrail_var.set("")
            self.exploration_var.set("")
            self.open_works_1_var.set("")
            self.open_works_2_var.set("")
            self.open_works_3_var.set("")
            self.lines_3d_ow_var.set("")
            self.lines_3d_ea_var.set("")
            
            # Clear data storage
            self.data.clear()
            self.results.clear()
            
            # Clear caches
            self.normalize_cache.clear()
            self.match_cache.clear()
            
            # Clear the cached normalize function
            self.normalize_string_cached.cache_clear()
            
            # Reset processing state
            self.processing_cancelled = False
            
            # Reset progress and status
            self.progress_var.set(0)
            self.status_var.set("Ready")
            self.perf_var.set("")
            
            # Clear results text area
            self.results_text.delete(1.0, tk.END)
            self.results_text.insert(tk.END, "All data cleared. Ready for new analysis.\n\n")
            
            # Reset button states
            self.process_button.config(state=tk.NORMAL)
            self.cancel_button.config(state=tk.DISABLED)
            self.export_button.config(state=tk.DISABLED)
            
            # Reset performance monitor
            self.performance_monitor = PerformanceMonitor()
            
            # Reset visualization tab
            self.create_visualization_placeholder()
            self.main_notebook.select(0)  # Switch back to processing tab
            
            # Force garbage collection to free memory
            import gc
            gc.collect()
            
            # Show confirmation
            self.status_var.set("All data cleared successfully")
            
            # Show memory freed information if available
            try:
                import psutil
                process = psutil.Process()
                current_memory = process.memory_info().rss / 1024 / 1024  # MB
                self.results_text.insert(tk.END, f"Memory usage after cleanup: {current_memory:.1f} MB\n")
                self.results_text.insert(tk.END, "Cache cleared, ready for new files.\n")
            except:
                pass
                
        except Exception as e:
            messagebox.showerror("Error", f"Failed to clear data: {str(e)}")
            self.status_var.set("Error during cleanup")
    
    def cancel_processing(self):
        """Cancel the current processing operation"""
        self.processing_cancelled = True
        self.update_ui_safe(lambda: self.status_var.set("Cancelling..."))
    
    def process_data_threaded(self):
        """Run data processing in a separate thread with proper thread safety"""
        self.processing_cancelled = False
        self.match_cache.clear()  # Clear cache for fresh processing
        
        def update_buttons(process_enabled, cancel_enabled, export_enabled):
            self.process_button.config(state=tk.NORMAL if process_enabled else tk.DISABLED)
            self.cancel_button.config(state=tk.NORMAL if cancel_enabled else tk.DISABLED)
            self.export_button.config(state=tk.NORMAL if export_enabled else tk.DISABLED)
        
        self.update_ui_safe(lambda: update_buttons(False, True, False))
        
        thread = threading.Thread(target=self.process_data)
        thread.daemon = True
        thread.start()
    
    def process_data(self):
        """Main data processing function with optimizations and performance monitoring"""
        try:
            # Start performance monitoring
            self.performance_monitor.start_monitoring()
            
            # Clear previous results
            self.update_ui_safe(lambda: self.results_text.delete(1.0, tk.END))
            self.results = {}
            
            # Validate input files
            self.update_ui_safe(lambda: self.status_var.set("Validating files..."))
            self.update_ui_safe(lambda: self.progress_var.set(5))
            
            try:
                validation_results = self.validate_input_files()
            except ValueError as e:
                self.update_ui_safe(lambda: messagebox.showerror("Validation Error", str(e)))
                return
            
            if self.processing_cancelled:
                return
            
            # Load FileTrail data
            self.update_ui_safe(lambda: self.status_var.set("Loading FileTrail data..."))
            self.update_ui_safe(lambda: self.progress_var.set(10))
            
            filetrail_data = self.load_data_efficiently(self.file_paths['filetrail'], max_rows=100000)
            if filetrail_data is None:
                return
            
            # Filter by HOME_LOCATION
            self.update_ui_safe(lambda: self.status_var.set("Filtering FileTrail data..."))
            self.update_ui_safe(lambda: self.progress_var.set(20))
            
            filtered_filetrail = filetrail_data[
                filetrail_data['HOME_LOCATION'].str.contains('PT-US-HOU-WCK', na=False, case=False)
            ].copy()
            
            total_filetrail = len(filetrail_data)
            filtered_count = len(filtered_filetrail)
            
            result_text = f"FileTrail Processing Results:\n"
            result_text += f"Total FileTrail records: {total_filetrail}\n"
            result_text += f"Records with PT-US-HOU-WCK: {filtered_count}\n\n"
            
            self.update_ui_safe(lambda: self.results_text.insert(tk.END, result_text))
            
            if self.processing_cancelled:
                return
            
            # Load comparison files efficiently
            self.update_ui_safe(lambda: self.status_var.set("Loading comparison files..."))
            self.update_ui_safe(lambda: self.progress_var.set(30))
            
            comparison_data = {}
            comparison_columns = {
                'exploration_archives': 'EA Line Name',
                'open_works_1': 'LINE',
                'open_works_2': 'LINE',
                'open_works_3': 'LINE',
                'lines_3d_ow': 'SURVEY',
                'lines_3d_ea': 'LINE_ID'
            }
            
            for file_type, column_name in comparison_columns.items():
                if self.file_paths[file_type] and validation_results.get(file_type) == "OK":
                    data = self.load_data_efficiently(self.file_paths[file_type], column_name, max_rows=50000)
                    if data is not None and column_name in data.columns:
                        comparison_data[file_type] = data[column_name].dropna().tolist()
                        display_name = self.get_display_name(file_type)
                        result_text = f"{display_name}: {len(comparison_data[file_type])} records\n"
                    else:
                        display_name = self.get_display_name(file_type)
                        result_text = f"{display_name}: Column '{column_name}' not found or file error\n"
                else:
                    display_name = self.get_display_name(file_type)
                    result_text = f"{display_name}: {validation_results.get(file_type, 'Not provided')}\n"
                
                self.update_ui_safe(lambda text=result_text: self.results_text.insert(tk.END, text))
                
                if self.processing_cancelled:
                    return
            
            self.update_ui_safe(lambda: self.results_text.insert(tk.END, "\n"))
            
            # Create optimized lookup structures
            self.update_ui_safe(lambda: self.status_var.set("Creating lookup structures..."))
            self.update_ui_safe(lambda: self.progress_var.set(40))
            self.update_performance_display()
            
            lookup_structures = self.create_lookup_structures(comparison_data)
            
            if self.processing_cancelled:
                return
            
            # Process matches with optimizations
            self.update_ui_safe(lambda: self.status_var.set("Processing matches..."))
            self.update_ui_safe(lambda: self.progress_var.set(50))
            
            # Initialize result columns
            match_columns = {
                'EA_Match': '', 'EA_Match_Type': '', 'EA_Score': 0,
                'OW1_Match': '', 'OW1_Match_Type': '', 'OW1_Score': 0,
                'OW2_Match': '', 'OW2_Match_Type': '', 'OW2_Score': 0,
                'OW3_Match': '', 'OW3_Match_Type': '', 'OW3_Score': 0,
                '3D_OW_Match': '', '3D_OW_Match_Type': '', '3D_OW_Score': 0,
                '3D_EA_Match': '', '3D_EA_Match_Type': '', '3D_EA_Score': 0,
                'Has_Any_Match': False
            }
            
            for col, default_val in match_columns.items():
                filtered_filetrail[col] = default_val
            
            # Track statistics and accuracy metrics
            stats = {
                'exploration_archives': {'exact': 0, 'substring': 0, 'fuzzy': 0, 'total': 0},
                'open_works_1': {'exact': 0, 'substring': 0, 'fuzzy': 0, 'total': 0},
                'open_works_2': {'exact': 0, 'substring': 0, 'fuzzy': 0, 'total': 0},
                'open_works_3': {'exact': 0, 'substring': 0, 'fuzzy': 0, 'total': 0},
                'lines_3d_ow': {'exact': 0, 'substring': 0, 'fuzzy': 0, 'total': 0},
                'lines_3d_ea': {'exact': 0, 'substring': 0, 'fuzzy': 0, 'total': 0}
            }
            
            match_details = []
            total_processed = 0
            total_with_matches = 0
            
            # Process in smaller batches with cancellation checks
            batch_size = 50
            total_rows = len(filtered_filetrail)
            
            for batch_start in range(0, total_rows, batch_size):
                if self.processing_cancelled:
                    self.update_ui_safe(lambda: self.status_var.set("Processing cancelled"))
                    return
                
                batch_end = min(batch_start + batch_size, total_rows)
                batch_progress = 50 + (batch_start / total_rows) * 30
                self.update_ui_safe(lambda prog=batch_progress: self.progress_var.set(prog))
                self.update_performance_display()
                
                # Process batch
                for idx in range(batch_start, batch_end):
                    row_idx = filtered_filetrail.index[idx]
                    line_id = filtered_filetrail.loc[row_idx, 'LINE_ID']
                    
                    if pd.isna(line_id):
                        continue
                    
                    total_processed += 1
                    
                    # Find matches using optimized lookup
                    matches = self.find_matches_optimized(line_id, lookup_structures)
                    
                    has_match = bool(matches)
                    if has_match:
                        total_with_matches += 1
                        
                    filtered_filetrail.loc[row_idx, 'Has_Any_Match'] = has_match
                    
                    # Store match results with mapping
                    file_type_mapping = {
                        'exploration_archives': ('EA_Match', 'EA_Match_Type', 'EA_Score'),
                        'open_works_1': ('OW1_Match', 'OW1_Match_Type', 'OW1_Score'),
                        'open_works_2': ('OW2_Match', 'OW2_Match_Type', 'OW2_Score'),
                        'open_works_3': ('OW3_Match', 'OW3_Match_Type', 'OW3_Score'),
                        'lines_3d_ow': ('3D_OW_Match', '3D_OW_Match_Type', '3D_OW_Score'),
                        'lines_3d_ea': ('3D_EA_Match', '3D_EA_Match_Type', '3D_EA_Score')
                    }
                    
                    for file_type, match_info in matches.items():
                        if file_type in stats:
                            stats[file_type][match_info['match_type']] += 1
                            stats[file_type]['total'] += 1
                        
                        # Store in appropriate columns
                        if file_type in file_type_mapping:
                            match_col, type_col, score_col = file_type_mapping[file_type]
                            filtered_filetrail.loc[row_idx, match_col] = match_info['value']
                            filtered_filetrail.loc[row_idx, type_col] = match_info['match_type']
                            filtered_filetrail.loc[row_idx, score_col] = match_info['score']
                        
                        # Store detailed match info
                        display_name = self.get_display_name(file_type)
                        match_details.append({
                            'FileTrail_LINE_ID': line_id,
                            'Database': display_name,
                            'Matched_Value': match_info['value'],
                            'Match_Type': match_info['match_type'],
                            'Score': match_info['score']
                        })
            
            if self.processing_cancelled:
                self.update_ui_safe(lambda: self.status_var.set("Processing cancelled"))
                return
            
            # Store results
            self.results['filtered_filetrail'] = filtered_filetrail
            self.results['match_details'] = pd.DataFrame(match_details)
            self.results['unique_records'] = filtered_filetrail[~filtered_filetrail['Has_Any_Match']].copy()
            
            # Perform match quality analysis
            self.update_ui_safe(lambda: self.status_var.set("Analyzing match quality..."))
            self.update_ui_safe(lambda: self.progress_var.set(85))
            
            analysis = self.match_analyzer.analyze_matches(filtered_filetrail)
            self.results['analysis'] = analysis
            
            # Calculate match accuracy metrics
            match_rate = (total_with_matches / total_processed * 100) if total_processed > 0 else 0
            
            # Display comprehensive statistics
            min_substring_length = self.substring_length_var.get()
            fuzzy_threshold = self.fuzzy_threshold_var.get()
            
            stats_text = f"\nMatch Statistics (min {min_substring_length} chars substring, {fuzzy_threshold}% fuzzy):\n"
            stats_text += f"Total records processed: {total_processed}\n"
            stats_text += f"Records with matches: {total_with_matches} ({match_rate:.1f}%)\n\n"
            
            for file_type, stat in stats.items():
                if stat['total'] > 0:
                    display_name = self.get_display_name(file_type)
                    stats_text += f"{display_name}: {stat['total']} matches\n"
                    stats_text += f"  - Exact: {stat['exact']} ({stat['exact']/stat['total']*100:.1f}%)\n"
                    stats_text += f"  - Substring: {stat['substring']} ({stat['substring']/stat['total']*100:.1f}%)\n"
                    stats_text += f"  - Fuzzy: {stat['fuzzy']} ({stat['fuzzy']/stat['total']*100:.1f}%)\n"
            
            unique_count = len(self.results['unique_records'])
            stats_text += f"\nUnique FileTrail records (no matches): {unique_count}\n"
            
            # Performance summary
            final_stats = self.performance_monitor.stop_monitoring()
            if final_stats:
                stats_text += f"\nPerformance Summary:\n"
                stats_text += f"Total processing time: {final_stats['elapsed_time']:.2f} seconds\n"
                stats_text += f"Peak memory usage: {final_stats['peak_memory']:.1f} MB\n"
                stats_text += f"Records processed per second: {total_processed/final_stats['elapsed_time']:.1f}\n"
                stats_text += f"Cache hit efficiency: {len(self.match_cache)} unique patterns cached\n"
            
            self.update_ui_safe(lambda: self.results_text.insert(tk.END, stats_text))
            
            # Create visualizations
            self.update_ui_safe(lambda: self.status_var.set("Creating visualizations..."))
            self.update_ui_safe(lambda: self.progress_var.set(95))
            
            self.update_ui_safe(lambda: self.create_visualizations(analysis))
            
            # Complete
            self.update_ui_safe(lambda: self.status_var.set("Processing complete!"))
            self.update_ui_safe(lambda: self.progress_var.set(100))
            self.update_ui_safe(lambda: self.export_button.config(state=tk.NORMAL))
            
        except Exception as e:
            error_msg = f"Processing failed: {str(e)}\n\nDetails:\n{traceback.format_exc()}"
            self.update_ui_safe(lambda: messagebox.showerror("Error", error_msg))
            self.update_ui_safe(lambda: self.status_var.set("Error occurred"))
        
        finally:
            # Reset buttons
            self.update_ui_safe(lambda: self.process_button.config(state=tk.NORMAL))
            self.update_ui_safe(lambda: self.cancel_button.config(state=tk.DISABLED))
    
    def export_to_excel(self):
        """Export results to Excel file with enhanced error handling and charts"""
        if not self.results:
            messagebox.showwarning("Warning", "No results to export!")
            return
        
        # Get output file path
        output_path = filedialog.asksaveasfilename(
            title="Save Results",
            defaultextension=".xlsx",
            filetypes=[("Excel files", "*.xlsx")]
        )
        
        if not output_path:
            return
        
        try:
            # Create a progress dialog for export
            self.status_var.set("Exporting to Excel...")
            self.progress_var.set(0)
            
            # Create temporary directory for charts
            import tempfile
            temp_dir = tempfile.mkdtemp()
            
            # Save charts for Excel
            chart_paths = {}
            if 'analysis' in self.results:
                self.progress_var.set(10)
                chart_paths = self.data_visualizer.save_charts_for_excel(self.results['analysis'], temp_dir)
            
            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                # Sheet 1: Unique FileTrail records
                self.progress_var.set(25)
                self.results['unique_records'].to_excel(
                    writer, sheet_name='Unique_FileTrail_Records', index=False
                )
                
                # Sheet 2: All FileTrail records with match info
                self.progress_var.set(40)
                self.results['filtered_filetrail'].to_excel(
                    writer, sheet_name='All_FileTrail_Records', index=False
                )
                
                # Sheet 3: Match details
                self.progress_var.set(55)
                if not self.results['match_details'].empty:
                    self.results['match_details'].to_excel(
                        writer, sheet_name='Match_Details', index=False
                    )
                
                # Sheet 4: Enhanced Summary with match quality analysis
                self.progress_var.set(70)
                summary_data = []
                if hasattr(self, 'results') and self.results and 'analysis' in self.results:
                    analysis = self.results['analysis']
                    total_records = analysis['total_records']
                    matched_records = analysis['records_with_matches']
                    
                    summary_data = [
                        ['Metric', 'Value'],
                        ['Total FileTrail Records Processed', total_records],
                        ['Records with Matches', matched_records],
                        ['Unique Records (No Matches)', total_records - matched_records],
                        ['Match Rate (%)', f"{(matched_records/total_records*100):.1f}" if total_records > 0 else "0"],
                        ['Processing Date', datetime.now().strftime('%Y-%m-%d %H:%M:%S')],
                        ['Substring Min Length', self.substring_length_var.get()],
                        ['Fuzzy Threshold (%)', self.fuzzy_threshold_var.get()],
                        ['', ''],  # Separator
                        ['Match Quality Breakdown', ''],
                        ['Perfect Matches (100%)', analysis['overall_categories']['Perfect']],
                        ['Excellent Matches (95-99%)', analysis['overall_categories']['Excellent']],
                        ['Very Good Matches (85-94%)', analysis['overall_categories']['Very Good']],
                        ['Good Matches (75-84%)', analysis['overall_categories']['Good']],
                        ['Fair Matches (65-74%)', analysis['overall_categories']['Fair']],
                        ['Poor Matches (50-64%)', analysis['overall_categories']['Poor']],
                        ['No Matches (0%)', analysis['overall_categories']['No Match']],
                        ['', ''],  # Separator
                        ['Match Type Breakdown', ''],
                        ['Exact Matches', analysis['match_types']['exact']],
                        ['Substring Matches', analysis['match_types']['substring']],
                        ['Fuzzy Matches', analysis['match_types']['fuzzy']],
                        ['No Matches', analysis['match_types']['no_match']]
                    ]
                
                if summary_data:
                    summary_df = pd.DataFrame(summary_data[1:], columns=summary_data[0])
                    summary_df.to_excel(writer, sheet_name='Analysis_Summary', index=False)
                
                # Sheet 5: Database breakdown
                self.progress_var.set(85)
                if 'analysis' in self.results:
                    db_breakdown_data = []
                    db_breakdown_data.append(['Database', 'Total Matches', 'Perfect', 'Excellent', 'Very Good', 'Good', 'Fair', 'Poor', 'No Match'])
                    
                    for db_name, db_stats in self.results['analysis']['database_breakdown'].items():
                        display_name = self.get_display_name(db_name)
                        row = [
                            display_name,
                            db_stats['total_matches'],
                            db_stats['categories']['Perfect'],
                            db_stats['categories']['Excellent'],
                            db_stats['categories']['Very Good'],
                            db_stats['categories']['Good'],
                            db_stats['categories']['Fair'],
                            db_stats['categories']['Poor'],
                            db_stats['categories']['No Match']
                        ]
                        db_breakdown_data.append(row)
                    
                    db_breakdown_df = pd.DataFrame(db_breakdown_data[1:], columns=db_breakdown_data[0])
                    db_breakdown_df.to_excel(writer, sheet_name='Database_Breakdown', index=False)
                
                # Add charts to Excel (if available)
                if chart_paths:
                    self.progress_var.set(95)
                    try:
                        from openpyxl.drawing.image import Image
                        
                        # Add charts to appropriate sheets
                        if 'quality_distribution' in chart_paths:
                            ws = writer.sheets['Analysis_Summary']
                            img = Image(chart_paths['quality_distribution'])
                            img.width = 600
                            img.height = 300
                            ws.add_image(img, 'E2')
                        
                        if 'database_comparison' in chart_paths:
                            ws = writer.sheets['Database_Breakdown']
                            img = Image(chart_paths['database_comparison'])
                            img.width = 700
                            img.height = 400
                            ws.add_image(img, 'K2')
                    except Exception as chart_error:
                        print(f"Warning: Could not add charts to Excel: {chart_error}")
            
            # Clean up temporary files
            try:
                import shutil
                shutil.rmtree(temp_dir)
            except:
                pass
            
            self.progress_var.set(100)
            self.status_var.set("Export completed!")
            messagebox.showinfo("Success", f"Enhanced results with visualizations exported to {output_path}")
            
        except Exception as e:
            messagebox.showerror("Error", f"Export failed: {str(e)}")
        finally:
            self.progress_var.set(0)
            self.status_var.set("Ready")


def main():
    """Main function to run the application"""
    # Check for required dependencies
    try:
        import matplotlib
        matplotlib.use('TkAgg')  # Use TkAgg backend for tkinter integration
    except ImportError:
        print("Error: matplotlib is required but not installed.")
        print("Install it using: pip install matplotlib")
        return
    
    root = tk.Tk()
    app = FileTrailComparator(root)
    
    # Handle window closing gracefully
    def on_closing():
        if hasattr(app, 'processing_cancelled'):
            app.processing_cancelled = True
        root.destroy()
    
    root.protocol("WM_DELETE_WINDOW", on_closing)
    root.mainloop()


if __name__ == "__main__":
    main()
